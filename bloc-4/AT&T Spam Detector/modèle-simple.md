# **mod√®le simple** de classification spam / ham avec PyTorch

---

## üß† Objectif du mod√®le

Tu veux **classer un message texte** en deux classes :

* `0 = ham` (non-spam)
* `1 = spam`

Mais comme tu ne pars **pas d‚Äôun mod√®le de langage pr√©-entra√Æn√©**, tu vectorises les messages avec **TF-IDF** (chaque mot devient un score selon son importance dans le texte), et tu donnes √ßa comme **entr√©e √† un r√©seau de neurones simple**.

---

## üîç Le mod√®le PyTorch

Voici le code du mod√®le :

```python
import torch.nn as nn

class SpamClassifier(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, 2)  # 2 classes : ham et spam
        )

    def forward(self, x):
        return self.model(x)
```

---

## üß± D√©tail de l‚Äôarchitecture

### 1. **`nn.Linear(input_dim, 128)`**

* C‚Äôest une **couche dense** (fully connected)
* Elle transforme l‚Äôentr√©e (un vecteur TF-IDF de dimension `input_dim`, ex: 5000) en un vecteur de taille 128
* Elle apprend **des pond√©rations** pour chaque mot

### 2. **`nn.ReLU()`**

* Fonction d‚Äôactivation : garde les valeurs positives, met les n√©gatives √† 0
* Ajoute de la **non-lin√©arit√©** ‚Üí plus de capacit√© d‚Äôapprentissage

### 3. **`nn.Dropout(0.3)`**

* Pour √©viter le surapprentissage (**drop 30%** des neurones pendant le training)

### 4. **`nn.Linear(128, 2)`**

* C‚Äôest la **couche de sortie**
* Elle produit un vecteur de taille 2 (1 score par classe)

> Ensuite, on utilise `CrossEntropyLoss` qui applique un **softmax + log-likelihood**, pour calculer la perte.

---

## üîÅ Forward Pass

* Le mod√®le prend un vecteur TF-IDF comme entr√©e (ex: `[0.1, 0.02, 0.0, ..., 0.4]`)
* Il le passe dans les couches
* Il retourne un **logit** (ex: `[1.4, 0.3]`)
* On prend `argmax` pour choisir la classe pr√©dite

---

## üéØ En r√©sum√©

| √âl√©ment            | R√¥le                                                     |
| ------------------ | -------------------------------------------------------- |
| `TF-IDF`           | Transforme le texte en vecteur num√©rique                 |
| `Linear`           | Apprend √† associer chaque mot √† une classe               |
| `ReLU`             | Permet au r√©seau d‚Äôapprendre des relations non lin√©aires |
| `Dropout`          | R√©gularise pour √©viter l‚Äôoverfitting                     |
| `CrossEntropyLoss` | Calcule la perte entre pr√©diction et v√©rit√©              |

Excellente question ! Beaucoup se la posent en classification binaire.
Voici la **diff√©rence claire entre `BinaryCrossEntropy` et `CrossEntropyLoss`**, et **pourquoi on utilise `CrossEntropyLoss` ici** üëá

---

# BinaryCrossEntropy vs CrossEntropyLoss

## ‚úÖ TL;DR

| Situation                    | Utilise             |
| ---------------------------- | ------------------- |
| 1 seule sortie (score spam)  | `BCEWithLogitsLoss` |
| 2 sorties (score ham + spam) | `CrossEntropyLoss`  |

---

## üß† Pourquoi tu utilises `CrossEntropyLoss`

Dans ton mod√®le :

```python
self.model = nn.Sequential(
    nn.Linear(input_dim, 128),
    nn.ReLU(),
    nn.Dropout(0.3),
    nn.Linear(128, 2)  # üîπ 2 sorties : ham et spam
)
```

‚Üí **Tu as 2 sorties logiques** (`output = [score_ham, score_spam]`)

Tu veux que le mod√®le choisisse la classe avec le **score le plus √©lev√©** ‚áí tu utilises :

```python
criterion = nn.CrossEntropyLoss()
```

‚úÖ `CrossEntropyLoss` :

* Combine automatiquement **`log_softmax` + `negative log likelihood`**
* Prend des **logits** (non normalis√©s)
* S‚Äôattend √† des **entiers comme labels** : `0` (ham), `1` (spam)

---

## ‚ùì Quand utiliser `BCEWithLogitsLoss` alors ?

Si tu fais une **classification binaire avec 1 seule sortie**, par exemple :

```python
self.model = nn.Sequential(
    nn.Linear(input_dim, 128),
    nn.ReLU(),
    nn.Linear(128, 1),  # üî∏ 1 seule sortie : probabilit√© que ce soit du spam
)
```

‚Üí Tu dois utiliser :

```python
criterion = nn.BCEWithLogitsLoss()
```

Et :

* La sortie doit passer par une **sigmo√Øde** (internement dans `BCEWithLogitsLoss`)
* Le label doit √™tre un **float** entre `0.` et `1.`

---

## ‚úÖ R√©sum√© rapide

| Architecture               | Perte √† utiliser      | Labels attendus |
| -------------------------- | --------------------- | --------------- |
| Sortie `nn.Linear(..., 2)` | `CrossEntropyLoss()`  | entiers `0/1`   |
| Sortie `nn.Linear(..., 1)` | `BCEWithLogitsLoss()` | float `0. / 1.` |

---

## üëâ Bonus : Multi-label ?

Si tu faisais de la **multi-label classification** (ex: un message peut √™tre √† la fois `spam` et `phishing`), alors tu aurais plusieurs sorties binaires ‚Üí l√† aussi on utilise `BCEWithLogitsLoss`.


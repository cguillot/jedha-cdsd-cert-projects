digraph {
	graph [size="12,12"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140596173456688 [label="
 (1)" fillcolor=darkolivegreen1]
	140596277405696 [label="SqueezeBackward1
----------------------
dim           :      1
self_sym_sizes: (1, 1)"]
	140596230782832 -> 140596277405696
	140596230782832 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :        (1, 64)
mat1_sym_strides:        (64, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :        (64, 1)
mat2_sym_strides:        (1, 64)"]
	140596173520416 -> 140596230782832
	140596173645264 [label="fc.2.bias
 (1)" fillcolor=lightblue]
	140596173645264 -> 140596173520416
	140596173520416 [label=AccumulateGrad]
	140596173513648 -> 140596230782832
	140596173513648 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140596173271248 -> 140596173513648
	140596173271248 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (1, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :      (128, 64)
mat2_sym_strides:       (1, 128)"]
	140596173259440 -> 140596173271248
	140596173645168 [label="fc.0.bias
 (64)" fillcolor=lightblue]
	140596173645168 -> 140596173259440
	140596173259440 [label=AccumulateGrad]
	140596173269424 -> 140596173271248
	140596173269424 [label="SqueezeBackward1
---------------------------
dim           :           2
self_sym_sizes: (1, 128, 1)"]
	140596173258864 -> 140596173269424
	140596173258864 [label="SqueezeBackward1
------------------------------------
dim           : 18446744073709551614
self_sym_sizes:       (1, 128, 1, 1)"]
	140596173269616 -> 140596173258864
	140596173269616 [label="AsStridedBackward1
----------------------------------
size          :     (1, 128, 1, 1)
storage_offset:               None
stride        : (128, 1, 128, 128)"]
	140596173271296 -> 140596173269616
	140596173271296 [label="MeanBackward1
------------------------------------------------------------
dim           : (18446744073709551615, 18446744073709551614)
keepdim       :                                         True
self_sym_numel:                                        16384
self_sym_sizes:                             (1, 128, 1, 128)"]
	140596173265440 -> 140596173271296
	140596173265440 [label="UnsqueezeBackward0
-------------------------
dim: 18446744073709551614"]
	140596173265392 -> 140596173265440
	140596173265392 [label="PermuteBackward0
----------------
dims: (0, 2, 1)"]
	140596173273984 -> 140596173265392
	140596173273984 [label="EmbeddingBackward0
------------------------------------
indices             : [saved tensor]
padding_idx         :              0
scale_grad_by_freq  :          False
sparse              :          False
weight_sym_argsize_0:         100277"]
	140596173266640 -> 140596173273984
	140596173644496 [label="embedding.weight
 (100277, 128)" fillcolor=lightblue]
	140596173644496 -> 140596173266640
	140596173266640 [label=AccumulateGrad]
	140596173273648 -> 140596173271248
	140596173273648 [label=TBackward0]
	140596173269952 -> 140596173273648
	140596173638928 [label="fc.0.weight
 (64, 128)" fillcolor=lightblue]
	140596173638928 -> 140596173269952
	140596173269952 [label=AccumulateGrad]
	140596173513888 -> 140596230782832
	140596173513888 [label=TBackward0]
	140596173270000 -> 140596173513888
	140596173639888 [label="fc.2.weight
 (1, 64)" fillcolor=lightblue]
	140596173639888 -> 140596173270000
	140596173270000 [label=AccumulateGrad]
	140596277405696 -> 140596173456688
	140596173637584 [label="
 (1, 1)" fillcolor=darkolivegreen3]
	140596230782832 -> 140596173637584
	140596173637584 -> 140596173456688 [style=dotted]
}
